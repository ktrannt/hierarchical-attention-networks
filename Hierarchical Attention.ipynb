{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "from tqdm import tqdm\n",
    "\n",
    "import ujson\n",
    "from data_util import batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up hyperparameters\n",
    "task = \"yelp\"\n",
    "mode = \"train\"    # train or eval\n",
    "checkpoint_frequency = 100\n",
    "eval_frequency = 10000\n",
    "batch_size = 30\n",
    "device =\"/gpu:0\"\n",
    "max_grad_norm =5.0 \n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = importlib.import_module(task_name)\n",
    "\n",
    "checkpoint_dir = os.path.join(task.train_dir, 'checkpoint')\n",
    "tflog_dir = os.path.join(task.train_dir, 'tflog')\n",
    "checkpoint_name = task_name + '-model'\n",
    "checkpoint_dir = os.path.join(task.train_dir, 'checkpoints')\n",
    "checkpoint_path = os.path.join(checkpoint_dir, checkpoint_name)\n",
    "\n",
    "# @TODO: move calculation into `task file`\n",
    "trainset = task.read_trainset(epochs=1)\n",
    "class_weights = pd.Series(Counter([l for _, l in trainset]))\n",
    "class_weights = 1/(class_weights/class_weights.mean())\n",
    "class_weights = class_weights.to_dict()\n",
    "\n",
    "vocab = task.read_vocab()\n",
    "labels = task.read_labels()\n",
    "\n",
    "classes = max(labels.values())+1\n",
    "vocab_size = task.vocab_size\n",
    "\n",
    "labels_rev = {int(v): k for k, v in labels.items()}\n",
    "vocab_rev = {int(v): k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HAN_model_1(session, restore_only=False):\n",
    "    \"\"\"Hierarhical Attention Network\"\"\"\n",
    "    import tensorflow as tf\n",
    "    try:\n",
    "    from tensorflow.contrib.rnn import GRUCell, MultiRNNCell, DropoutWrapper\n",
    "    except ImportError:\n",
    "    MultiRNNCell = tf.nn.rnn_cell.MultiRNNCell\n",
    "    GRUCell = tf.nn.rnn_cell.GRUCell\n",
    "    from bn_lstm import BNLSTMCell\n",
    "    from HAN_model import HANClassifierModel\n",
    "\n",
    "    is_training = tf.placeholder(dtype=tf.bool, name='is_training')\n",
    "\n",
    "    cell = BNLSTMCell(80, is_training) # h-h batchnorm LSTMCell\n",
    "    # cell = GRUCell(30)\n",
    "    cell = MultiRNNCell([cell]*5)\n",
    "\n",
    "    model = HANClassifierModel(\n",
    "      vocab_size=vocab_size,\n",
    "      embedding_size=200,\n",
    "      classes=classes,\n",
    "      word_cell=cell,\n",
    "      sentence_cell=cell,\n",
    "      word_output_size=100,\n",
    "      sentence_output_size=100,\n",
    "      device=args.device,\n",
    "      learning_rate=args.lr,\n",
    "      max_grad_norm=args.max_grad_norm,\n",
    "      dropout_keep_proba=0.5,\n",
    "      is_training=is_training,\n",
    "    )\n",
    "\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    checkpoint = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "    if checkpoint:\n",
    "    print(\"Reading model parameters from %s\" % checkpoint.model_checkpoint_path)\n",
    "    saver.restore(session, checkpoint.model_checkpoint_path)\n",
    "    elif restore_only:\n",
    "    raise FileNotFoundError(\"Cannot restore model\")\n",
    "    else:\n",
    "    print(\"Created model with fresh parameters\")\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    # tf.get_default_graph().finalize()\n",
    "    return model, saver\n",
    "\n",
    "    model_fn = HAN_model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(ex):\n",
    "    print('text: ' + '\\n'.join([' '.join([vocab_rev.get(wid, '<?>') for wid in sent]) for sent in ex[0]]))\n",
    "    print('label: ', labels_rev[ex[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iterator(dataset, batch_size, max_epochs):\n",
    "    for i in range(max_epochs):\n",
    "    xb = []\n",
    "    yb = []\n",
    "    for ex in dataset:\n",
    "        x, y = ex\n",
    "        xb.append(x)\n",
    "        yb.append(y)\n",
    "        if len(xb) == batch_size:\n",
    "        yield xb, yb\n",
    "        xb, yb = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ev(session, model, dataset):\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    examples = []\n",
    "    for x, y in tqdm(batch_iterator(dataset, args.batch_size, 1)):\n",
    "    examples.extend(x)\n",
    "    labels.extend(y)\n",
    "    predictions.extend(session.run(model.prediction, model.get_feed_data(x, is_training=False)))\n",
    "\n",
    "    df = pd.DataFrame({'predictions': predictions, 'labels': labels, 'examples': examples})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataset):\n",
    "    tf.reset_default_graph()\n",
    "    config = tf.ConfigProto(allow_soft_placement=True)\n",
    "    with tf.Session(config=config) as s:\n",
    "    model, _ = model_fn(s, restore_only=True)\n",
    "    df = ev(s, model, dataset)\n",
    "    print((df['predictions'] == df['labels']).mean())\n",
    "    import IPython\n",
    "    IPython.embed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    config = tf.ConfigProto(allow_soft_placement=True)\n",
    "\n",
    "    with tf.Session(config=config) as s:\n",
    "    model, saver = model_fn(s)\n",
    "    summary_writer = tf.summary.FileWriter(tflog_dir, graph=tf.get_default_graph())\n",
    "\n",
    "    # Format: tensorflow/contrib/tensorboard/plugins/projector/projector_config.proto\n",
    "    # pconf = projector.ProjectorConfig()\n",
    "\n",
    "    # # You can add multiple embeddings. Here we add only one.\n",
    "    # embedding = pconf.embeddings.add()\n",
    "    # embedding.tensor_name = m.embedding_matrix.name\n",
    "\n",
    "    # # Link this tensor to its metadata file (e.g. labels).\n",
    "    # embedding.metadata_path = vocab_tsv\n",
    "\n",
    "    # print(embedding.tensor_name)\n",
    "\n",
    "    # Saves a configuration file that TensorBoard will read during startup.\n",
    "\n",
    "    for i, (x, y) in enumerate(batch_iterator(task.read_trainset(epochs=3), args.batch_size, 300)):\n",
    "        fd = model.get_feed_data(x, y, class_weights=class_weights)\n",
    "\n",
    "        # import IPython\n",
    "        # IPython.embed()\n",
    "\n",
    "        t0 = time.clock()\n",
    "        step, summaries, loss, accuracy, _ = s.run([\n",
    "          model.global_step,\n",
    "          model.summary_op,\n",
    "          model.loss,\n",
    "          model.accuracy,\n",
    "          model.train_op,\n",
    "        ], fd)\n",
    "        td = time.clock() - t0\n",
    "\n",
    "        summary_writer.add_summary(summaries, global_step=step)\n",
    "        # projector.visualize_embeddings(summary_writer, pconf)\n",
    "\n",
    "        if step % 1 == 0:\n",
    "        print('step %s, loss=%s, accuracy=%s, t=%s, inputs=%s' % (step, loss, accuracy, round(td, 2), fd[model.inputs].shape))\n",
    "        if step != 0 and step % args.checkpoint_frequency == 0:\n",
    "        print('checkpoint & graph meta')\n",
    "        saver.save(s, checkpoint_path, global_step=step)\n",
    "        print('checkpoint done')\n",
    "        if step != 0 and step % args.eval_frequency == 0:\n",
    "        print('evaluation at step %s' % i)\n",
    "        dev_df = ev(s, model, task.read_devset(epochs=1))\n",
    "        print('dev accuracy: %.2f' % (dev_df['predictions'] == dev_df['labels']).mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
